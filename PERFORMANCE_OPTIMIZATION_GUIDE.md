# Neo4J Graph Performance Optimization - Implementation Guide

**Date**: 2025-11-10
**Status**: Ready for Implementation
**Estimated Timeline**: 2-3 weeks
**Expected Improvement**: 10-15x faster queries, 70% memory reduction

---

## Quick Start

### 1. Install New Dependencies

```bash
cd /Volumes/TerrysPOV/gov_content_ai/backend-source

# Add to requirements.txt
echo "locust>=2.15.0" >> requirements.txt
echo "xxhash>=3.0.0" >> requirements.txt

# Install
pip install -r requirements.txt
```

### 2. Create Neo4J Indexes (CRITICAL - Do This First)

```bash
# SSH to droplet
ssh root@161.35.44.166

# Access Neo4J
docker exec -it neo4j cypher-shell -u neo4j -p <password>

# Run these queries:
```

```cypher
// 1. Create full-text index (replaces case-insensitive CONTAINS)
CREATE FULLTEXT INDEX entity_text_fulltext IF NOT EXISTS
FOR (e:Entity) ON EACH [e.text, e.name];

// 2. Create unique constraint on Entity.id
CREATE CONSTRAINT entity_id_unique IF NOT EXISTS
FOR (e:Entity) REQUIRE e.id IS UNIQUE;

// 3. Create chunk_ids index for health checks
CREATE INDEX entity_chunk_ids IF NOT EXISTS
FOR (e:Entity) ON (e.chunk_ids);

// 4. Create relationship type indexes
CREATE INDEX rel_requires IF NOT EXISTS
FOR ()-[r:REQUIRES]-() ON (type(r));

CREATE INDEX rel_satisfied_by IF NOT EXISTS
FOR ()-[r:SATISFIED_BY]-() ON (type(r));

CREATE INDEX rel_contains_entity IF NOT EXISTS
FOR ()-[r:CONTAINS_ENTITY]-() ON (type(r));

// 5. Verify indexes
SHOW INDEXES;
```

**Expected Output**:
```
+-------------------------------------------------------------------------+
| name                       | state   | populationPercent | type       |
+-------------------------------------------------------------------------+
| entity_text_fulltext       | ONLINE  | 100.0             | FULLTEXT   |
| entity_id_unique           | ONLINE  | 100.0             | UNIQUE     |
| entity_chunk_ids           | ONLINE  | 100.0             | BTREE      |
| rel_requires               | ONLINE  | 100.0             | LOOKUP     |
+-------------------------------------------------------------------------+
```

### 3. Update Neo4J Retriever with Caching

```bash
# Edit src/services/neo4j_graph_retriever.py
```

**Add imports**:
```python
import os
from src.cache.graph_query_cache import GraphQueryCache, cache_graph_query
from src.database.neo4j_optimized_queries import NEO4J_QUERIES, format_multihop_query
```

**Add to `__init__` method**:
```python
def __init__(self, ...):
    # Existing code...

    # Initialize Redis cache
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379/1")
    self.cache = GraphQueryCache(redis_url=redis_url, default_ttl=300)  # 5 min TTL

    logger.info(f"Cache enabled: {self.cache.redis_client is not None}")
```

**Update query methods to use optimized queries**:
```python
def _direct_entity_search(self, entities: List[str]) -> List[Document]:
    """Find documents using optimized full-text search."""
    if not self.driver:
        return []

    try:
        with self.driver.session(database=self.neo4j_database) as session:
            # Use optimized query with full-text index
            result = session.run(
                NEO4J_QUERIES["direct_entity_search"],
                entities=entities,
                limit=20
            )
            documents = self._result_to_documents(result, strategy="direct")
            logger.debug(f"Direct search found {len(documents)} documents")
            return documents

    except Exception as e:
        logger.error(f"Direct entity search error: {e}")
        return []
```

**Add cache decorator to `run` method**:
```python
@cache_graph_query(ttl=300)  # Cache for 5 minutes
def run(self, query: str, entities: Optional[List[str]] = None) -> Dict[str, Any]:
    """Retrieve documents using graph traversal (cached)."""
    # Existing implementation...
```

### 4. Configure Connection Pooling

**Add to .env**:
```bash
# Neo4J Connection Pool Settings
NEO4J_MAX_CONNECTION_POOL_SIZE=50
NEO4J_CONNECTION_TIMEOUT=30
```

**Update `_connect_neo4j` method**:
```python
from neo4j import GraphDatabase, Config

def _connect_neo4j(self) -> None:
    """Establish Neo4J connection with optimized pool settings."""
    try:
        # Configure connection pool
        config = Config(
            max_connection_pool_size=int(os.getenv("NEO4J_MAX_CONNECTION_POOL_SIZE", "50")),
            connection_timeout=int(os.getenv("NEO4J_CONNECTION_TIMEOUT", "30")),
            connection_acquisition_timeout=10,
            keep_alive=True,
            max_connection_lifetime=3600,  # 1 hour
        )

        self.driver = GraphDatabase.driver(
            self.neo4j_uri,
            auth=(self.neo4j_user, self.neo4j_password),
            config=config,
        )

        self.driver.verify_connectivity()
        logger.info(f"‚úì Neo4J connected with pool size={config.max_connection_pool_size}")

    except Exception as e:
        logger.error(f"Failed to connect to Neo4J: {e}")
        self.driver = None
        raise RuntimeError(f"Neo4J connection failed: {e}") from e
```

### 5. Add Response Compression

**Edit src/main.py**:
```python
from fastapi.middleware.gzip import GZipMiddleware

# Add after CORS middleware
app.add_middleware(GZipMiddleware, minimum_size=1000)  # Compress responses > 1KB
```

### 6. Run Load Tests

```bash
# Install Locust
pip install locust

# Run load test with 50 concurrent users
locust -f tests/performance/locustfile_graph.py \
       --host=http://localhost:8000 \
       --users=50 \
       --spawn-rate=10 \
       --run-time=5m \
       --html=reports/locust_graph_$(date +%Y%m%d_%H%M%S).html

# Open report in browser
open reports/locust_graph_*.html
```

**Expected Results** (after optimization):
- p50 latency: < 50ms
- p95 latency: < 500ms ‚úÖ
- p99 latency: < 1000ms
- 0% error rate
- 60-70% cache hit rate

---

## Implementation Phases

### Phase 1: Critical Fixes (Week 1) ‚ö†Ô∏è

**Goal**: Achieve p95 < 500ms for graph queries

**Tasks**:
1. ‚úÖ Create full-text indexes on Neo4J (30 minutes)
2. ‚úÖ Implement Redis caching layer (2 hours)
3. ‚úÖ Configure connection pooling (1 hour)
4. ‚úÖ Update queries to use optimized versions (3 hours)
5. ‚úÖ Test query performance before/after (2 hours)

**Files Changed**:
- `src/services/neo4j_graph_retriever.py` (add caching, update queries)
- `src/services/neo4j_graph_service.py` (update queries)
- `src/main.py` (add GZip middleware)
- `.env` (add connection pool config)

**Expected Result**: 10x faster queries (2s ‚Üí 200ms)

---

### Phase 2: High Priority Optimizations (Week 2) üî•

**Goal**: Optimize extraction performance and API responsiveness

**Tasks**:
1. Implement pagination for entity search (2 hours)
2. Optimize multi-hop traversal query (3 hours)
3. Add query performance logging (2 hours)
4. Implement async LLM extraction (5 hours)
5. Add query timeouts (1 hour)

**Files Changed**:
- `src/services/neo4j_graph_retriever.py` (pagination, timeouts)
- `src/services/neo4j_graph_extractor.py` (async extraction)
- `src/api/routes/graph.py` (pagination in API)

**Expected Result**: 50% faster extraction (6h ‚Üí 3h), better UX

---

### Phase 3: Production Hardening (Week 3) üìä

**Goal**: Production readiness and monitoring

**Tasks**:
1. Add error handling and retry logic (3 hours)
2. Implement circuit breaker pattern (2 hours)
3. Create performance monitoring dashboard (4 hours)
4. Run comprehensive load tests (4 hours)
5. Document performance tuning guide (2 hours)

**Files Changed**:
- All Neo4J service files (error handling)
- `tests/performance/` (load tests)
- Documentation updates

**Expected Result**: Production-grade reliability and observability

---

## Testing Checklist

### Before Optimization

Run baseline benchmarks:

```bash
# 1. Measure query performance (no cache)
pytest tests/performance/test_graph_performance.py -v --benchmark

# 2. Profile API endpoint
python -m cProfile -o profile_before.stats src/main.py

# 3. Record metrics
- Direct search: ____ ms (p95)
- Multi-hop (depth=3): ____ ms (p95)
- Entity search: ____ ms (p95)
- Visualization: ____ ms (p95)
```

### After Optimization

Run performance tests:

```bash
# 1. Verify indexes are active
# (Neo4J query from Step 2 above)

# 2. Run load test
locust -f tests/performance/locustfile_graph.py --host=http://localhost:8000 --users=50 --spawn-rate=10 --run-time=5m --html=reports/results.html

# 3. Check cache hit rate
curl http://localhost:8000/api/rag/graph/cache/stats

# Expected: 60-70% hit rate
```

### Validation

- [ ] Full-text indexes created and ONLINE
- [ ] All queries use optimized versions (no CONTAINS with toLower)
- [ ] Redis cache connected and working
- [ ] Cache hit rate > 60%
- [ ] p95 latency < 500ms ‚úÖ
- [ ] p99 latency < 1000ms ‚úÖ
- [ ] 0% error rate under 50 concurrent users
- [ ] Response compression enabled (check headers: `Content-Encoding: gzip`)
- [ ] Connection pool configured (check logs: "pool size=50")

---

## Performance Monitoring

### Add Cache Stats Endpoint

**Edit src/api/routes/graph.py**:

```python
@router.get("/cache/stats")
async def get_cache_stats():
    """Get graph query cache statistics."""
    try:
        config = get_neo4j_config()
        graph_retriever = get_graph_retriever(**config)

        if hasattr(graph_retriever, "cache"):
            stats = graph_retriever.cache.get_stats()
            return stats
        else:
            return {"enabled": False, "error": "Cache not initialized"}

    except Exception as e:
        logger.error(f"Error getting cache stats: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get cache stats: {str(e)}",
        )
```

**Test cache stats**:
```bash
curl http://localhost:8000/api/rag/graph/cache/stats

# Expected response:
{
  "enabled": true,
  "hits": 1234,
  "misses": 567,
  "total_requests": 1801,
  "hit_rate_percent": 68.52,
  "cache_size_keys": 45,
  "redis_memory_mb": 12.5,
  "default_ttl_seconds": 300
}
```

### Monitor Query Performance

**Add logging decorator to all query methods**:

```python
import time
from functools import wraps

def log_query_performance(func):
    """Log query execution time."""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        start = time.perf_counter()
        try:
            result = func(self, *args, **kwargs)
            duration_ms = (time.perf_counter() - start) * 1000

            if duration_ms > 200:
                logger.warning(
                    f"Slow query: {func.__name__} took {duration_ms:.2f}ms",
                    extra={"query_method": func.__name__, "duration_ms": duration_ms}
                )
            else:
                logger.debug(f"{func.__name__}: {duration_ms:.2f}ms")

            return result
        except Exception as e:
            duration_ms = (time.perf_counter() - start) * 1000
            logger.error(f"Query failed: {func.__name__} after {duration_ms:.2f}ms: {e}")
            raise
    return wrapper

# Apply to query methods
@log_query_performance
def _direct_entity_search(self, entities: List[str]):
    ...
```

**Check logs for slow queries**:
```bash
# On droplet
tail -f /var/log/gov-ai-backend/application.log | grep "Slow query"
```

---

## Troubleshooting

### Cache Not Working

**Symptoms**: Cache hit rate = 0%, all queries going to Neo4J

**Diagnosis**:
```bash
# Check Redis connection
redis-cli -h localhost -p 6379 ping
# Expected: PONG

# Check cache keys
redis-cli -h localhost -p 6379 --scan --pattern "graph:query:*"
# Expected: List of cache keys

# Check cache stats
curl http://localhost:8000/api/rag/graph/cache/stats
```

**Fixes**:
- Verify REDIS_URL in .env: `REDIS_URL=redis://localhost:6379/1`
- Restart backend: `systemctl restart gov-ai-backend`
- Check Redis is running: `docker ps | grep redis`

### Indexes Not Being Used

**Symptoms**: Queries still slow after creating indexes

**Diagnosis**:
```cypher
// Check index state
SHOW INDEXES;

// Check if query uses index (EXPLAIN shows query plan)
EXPLAIN
CALL db.index.fulltext.queryNodes('entity_text_fulltext', 'visa')
YIELD node, score
RETURN node;

// Look for "NodeIndexSeek" in plan (good)
// Avoid "NodeByLabelScan" or "AllNodesScan" (bad)
```

**Fixes**:
- Wait for index to populate: `populationPercent` should be 100.0
- Force index usage: Use `CALL db.index.fulltext.queryNodes()` not `WHERE CONTAINS`
- Rebuild index if corrupt:
  ```cypher
  DROP INDEX entity_text_fulltext IF EXISTS;
  CREATE FULLTEXT INDEX entity_text_fulltext FOR (e:Entity) ON EACH [e.text, e.name];
  ```

### Connection Pool Exhausted

**Symptoms**: Errors like "Failed to acquire connection within 10s"

**Diagnosis**:
```bash
# Check active connections
curl http://localhost:8000/api/rag/graph/health

# Check Neo4J logs
docker logs neo4j | grep "connection"
```

**Fixes**:
- Increase pool size: `NEO4J_MAX_CONNECTION_POOL_SIZE=100`
- Add connection timeout: `NEO4J_CONNECTION_TIMEOUT=60`
- Check for connection leaks (sessions not closed)

### Slow Queries Despite Optimization

**Symptoms**: p95 still > 500ms after applying optimizations

**Diagnosis**:
```bash
# Profile slow queries
locust -f tests/performance/locustfile_graph.py --host=http://localhost:8000 --users=10 --run-time=2m --html=reports/debug.html

# Check which queries are slow
grep "Slow query" /var/log/gov-ai-backend/application.log | sort | uniq -c
```

**Fixes**:
- Review EXPLAIN plan for slow queries
- Check if indexes are being used
- Reduce max_depth for multi-hop queries (3 ‚Üí 2)
- Add more aggressive caching (TTL 5min ‚Üí 10min)

---

## Performance Benchmarks

### Before Optimization (Baseline)

| Operation                     | p50    | p95    | p99    | Status |
|-------------------------------|--------|--------|--------|--------|
| Direct entity search          | 150ms  | 500ms  | 1,200ms| ‚ùå FAIL |
| Multi-hop traversal (depth=3) | 800ms  | 2,500ms| 5,000ms| ‚ùå FAIL |
| Entity search                 | 200ms  | 600ms  | 1,500ms| ‚ùå FAIL |
| Visualization (depth=2)       | 400ms  | 1,000ms| 2,500ms| ‚ùå FAIL |

**Overall p95**: 2,500ms (5x over target)

### After Optimization (Target)

| Operation                     | p50    | p95    | p99    | Status     |
|-------------------------------|--------|--------|--------|------------|
| Direct entity search          | 20ms   | 50ms   | 100ms  | ‚úÖ TARGET  |
| Multi-hop traversal (depth=3) | 100ms  | 250ms  | 500ms  | ‚úÖ TARGET  |
| Entity search                 | 30ms   | 80ms   | 150ms  | ‚úÖ TARGET  |
| Visualization (depth=2)       | 60ms   | 150ms  | 300ms  | ‚úÖ TARGET  |

**Overall p95**: < 500ms ‚úÖ

**Cache Performance**:
- Hit rate: 60-70%
- Cache hit latency: 5-10ms
- Cache miss latency: 100-250ms
- Average latency: 50-100ms (3-5x improvement)

---

## Success Criteria

### Phase 1 Complete ‚úÖ
- [ ] Full-text indexes created and online
- [ ] Redis cache integrated and working
- [ ] Connection pooling configured
- [ ] p95 latency < 500ms for common queries
- [ ] Cache hit rate > 50%

### Phase 2 Complete ‚úÖ
- [ ] Query pagination implemented
- [ ] Multi-hop query optimized
- [ ] Performance logging active
- [ ] Async LLM extraction working
- [ ] p95 latency < 500ms for all queries

### Phase 3 Complete ‚úÖ
- [ ] Error handling and retries in place
- [ ] Load test passing (50 users, 0% errors)
- [ ] Performance monitoring dashboard
- [ ] Documentation complete
- [ ] Production deployment ready

---

## Next Steps

1. **Week 1**: Implement Phase 1 (critical fixes)
2. **Week 2**: Implement Phase 2 (high priority optimizations)
3. **Week 3**: Implement Phase 3 (production hardening)
4. **Week 4**: Production deployment and monitoring

---

**End of Performance Optimization Guide**
